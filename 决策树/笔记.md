## 决策树

### 1.熵
+ 信息熵：判定信息的大小
+ 单位：比特 bit
+ 公式：H(U) = E[-logpi]=-Σ(i=1~n)pilogpi,式中对数一般取2为底。
  + n代表的是随机变量D中的水平个数，pi表示随机变量的水平概率。
+ 用于衡量随机变量的混乱程度，变量的不确定性越大，信息熵越大。

### 2.条件熵
+ 条件熵：H(X|Y)表示在已知随机变量Y的条件下，随机变量X的不确定性。
+ 公式：H(X|Y)=-Σ(xy)p(x,y)log(p(x|y))

### 3.ID3算法
+ 用于选择决策树根节点问题
+ 信息获取量：通过特征字段作为节点分类获取了多少信息 
+ 选取Gain最大的最为根节点，根据其不同来分支，对于新的节点重新计算Gain以此类推，直到划分到所有样本属于同一类为止。（已经使用过的节点后续不再出现）
> Gain(A) = info(D) - info_A(D）
>    
> 注意点： 
> 1.输入的变量必须是分类变量，连续变量必须离散化。
> 2.info(D)信息熵，info(A)条件熵。
>
>缺点：
>1.倾向于选择水平数量较多的变量作为最重要的变量。
>2.输入的变量必须是分类变量，连续变量必须离散化。

### 4. C4.5算法
1.对比：
|  |首要变量筛选指标|特点|缺点  |
|:-|:-:|:-:|-:|
|ID3|信息增量|连续变量离散化|1.倾向于选择水平数量较多的变量作为最重要的变量。  2.输入的变量必须是分类变量，连续变量必须离散化。|
|C4.5|信息增益率|自动离散化，不需要分析师手动实现|
|CART|基尼系数| | |

2.信息增益率：
GainRate(D|A) = Gain(D|A)/info(A)
> Gain(D|A)在自变量A的条件下，目标变量D的信息增益。
> info(A)自变量A的信息熵。
> GainRate(D|A)在自变量A的条件下，目标变量D的信息增益率。

### CART回归树
+ 定义：用二叉树将预测空间递归划分为若干个子集，随着从根节点到叶节点的移动，从每一个节点中选择最优的分支规则对应的划分区域。
+ 目标使用：分类，数值预测
+ 分类指标：基尼系数
+ 公式：G = 1- Σ(i=1~k)pi^2
  + k:数据集中样本类型数量
  + pi:第i类样本的数量占总样本数量的比例
+ 多分问题：CART算法是使用的二叉树，对于对分类输入变量，首先需要将多类别合并成两个类别，形成超类。

> 基尼系数的性质与信息熵一样：度量随机变量的不确定度的大小：    
> 1.G越大，数据的不确定性越高;   
> 2.G越小，数据的不确定性越低;   
> 3.G=0，数据集中的所有样本都是同一类别。   

